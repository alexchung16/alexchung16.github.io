# 注意力机制（Attention Mechanism）

## 为什么需要注意力机制

神经网络中可以存储的信息量称为**网络容量（ Network Capacity）**  

> 利用一组神经元来存储信息时， 其存储容量和神经元的数量以及网络的复杂度成正比.  要存储的信息越多， 神经元数量就要越多或者网络要越复杂， 进而导致神经网络的参数成倍地增加．
>
> 人脑的生物神经网络同样存在网络容量问题， 人脑中的工作记忆大概只有几秒钟的时间， 类似于循环神经网络中的隐状态．人脑在有限的资源下， 并不能同时处理这些过载的输入信息． 大脑神经系统有两个重要机制可以解决信息过载问题： 注意力和记忆机制.    

通过借鉴人脑解决信息过载的机制， 从两方面来提高神经网络处理信息的能力． **一方面是注意力， 通过自上而下的信息选择机制来过滤掉大量的无关信息**； **另一方面是引入额外的外部记忆， 优化神经网络的记忆结构来提高神经网络存储信息的容量**. 

## 概念

>  **注意力机制也可称为注意力模型**．
>
> 在计算能力有限的情况下，注意力机制（ Attention Mechanism） 作为一种资源分配方案， 将有限的计算资源用来处理更重要的信息， 是解决信息超载问题的主要手段． 

 

注意力机制的计算可以分为两步： 

* 一是在所有输入信息上计算注意力分布
*  二是根据注意力分布来计算输入信息的加权平均．  

## 参考资料

* <https://medium.com/heuritech/attention-mechanism-5aba9a2d4727>

* <http://colah.github.io/posts/2015-08-Understanding-LSTMs/>

