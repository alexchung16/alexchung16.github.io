# 算法面试题目



## 数据结构



## 概率论

1. 54 张牌， 分成6份， 每张9张牌， 大小王在一起的概率

   $\frac{C_6^1 C_{52}^{4}}{C_{54}{6}}$

2. 一张硬币， 扔了一亿次正面都朝上， 再仍一次反面朝上的概率是多少

   $\frac {1}{100000000+2}$

3.  50 个红球和50个蓝球， 放入两个箱子， 怎样放置才能使得拿到的红球的概率最大

   $\frac{1}{2} + \frac{1}{2} \frac{49}{99}$

4. 将一根木棍分层三段，这三段构成三角形的概率

   0.25

5. 一个公交站再一分钟内有车经过的概率是p, 问三分钟内有车经过的概率

   1. $ 1 - (1-p)^3$

6. 一个小兵生命力为15， 现在每次攻击早曾的伤害在0到10之间，求两次攻击能够打死小兵的概率i|A

   $\frac{1}{4}$

7. 什么是马尔可夫链

   马尔可夫链描述的是随机变量的一个状态序列， 在这个状态序列里， 未来信息只与当前信息有关， 而与过去的信息无关。它有两个重要的假设:

   * t+1 时刻的状态概率分布只与t 时刻有关
   * t 到 t+1 时刻的状态转移与 t 时刻的状态值无关

   一个 马尔可夫链可以看作是 状态空间（所有可能的状态） + 状态转移矩阵 （条件概率分布）+ 初始概率分布（初始状态）

8. 什么是正态分布

   正态分布又称高斯分布， 他是连续型随机变量的分布， 主要由两个参数$u$ 和$\sigma^2$代表期望和方差。

   正态分布的规律： 取值里 $u$ 越近的概率越大， 同时 $\sigma$ 描述了分布的集中程度， $\sigma$ 越大，概率密度曲线越胖，  $\sigma$ 越小，概率密度曲线越矮

9. 全概率公式和贝叶斯公式区别

   全概率公式：事件A 发生的所有可能的原因是 $B_i（i=1，\dots, n）$, 、在事件 A 未发生的情况下， 已知 原因 B 的概率$P(B_i)$ 和 $P(B_i)$发生的情况下 A 发生的概率$P(A|B_i)$, 求 事件 A 发生的概率

   贝叶斯公式：在事件A已发生的情况下， 求导致 A 发生的各种原因 $B_i$的概率， 即$P(B_i|A)$

10. 什么是大数定理

    通俗地说就是， 样本数量很大的时候， 样本均值和数据期望充分接近，也就是说当我们大量重复某一相同的实验时， 其最后的实验结果可能会稳定在某一数值附近。

    

## 计算机视觉

### 什么是感受野

###  卷积层输出尺寸、参数量和计算量的计算  

### 什么是分组卷积

### 空洞卷积/膨胀卷积 的作用

###  转置卷积的主要思想 

### 动态卷积

### 解释 残差模块和它的作用

### Batch Normalize 与 Layer Normalize 区别

### dropout 作用

### iou 与 giou 区别

### 目标检测中 one-stage 与 two-stage 区别

### 目标检测、 语义分割、实例分割、全景分割区别

### 简单介绍几种常见的轻量级网络

### 简单介绍 tow-stage 模型 RCNN Fast RCNN  Faster RCNN 的发展过程

### 简单介绍yolo yolov2 yolov3 yolov4 的发展过程

### map 的概念和计算过程

### 介绍一下 计算机视觉中的注意力机制 

### 介绍一下 1x1 卷积的作用

### NMS 与 soft-NMS 的区别

### 目标检测中样本不平衡问题的的处理方法

样本不平衡问题可以分为正负样本不平衡和难易样本不平衡两个方面。

正负样本不平衡是指， 正样本是指图片中感兴趣的目标区域， 负样本是指背景区域。对于一张图片中负样本的数量会远远多于正样本， 造成正负样本的比例不平衡，  

* 一般在目标检测框架中保持正负样本的比例为1:3。
*  ATSS.

关于难易样本不均衡， 易分样本是指容易被正确分类的样本，易分样本包含易分正样本和易分负样本；难样本主要是指错分样本，难分样本包含难分正样本和难分负样本。易分样本在所有的样本中占大多数， 这就是难易样本不平衡。 易分样本（指置信度高的样本）单个样本的损失函数比较小， 对模型的提升效果比较小， 而错分样本的损失函数比较大， 对模型的提升更具有意义。 易分样本数量在总体样本中占有绝对优势，即使单个样本的损失函数较小，但是累积的损失函数会主导损失函数，而这部分样本本身就能被模型很好地分类，所以这部分引导的参数更新并不会改善模型的判断能力。难分样本在训练过程中单个样本的损失函数较高，具有多样性，但是该类占总体样本的比例较小。这会导致训练效率变得很低，甚至模型不能收敛。

* OHEM

  OHEM算法（online hard example miniing，发表于2016年的CVPR）主要是针对训练过程中的困难样本自动选择，其核心思想是根据输入样本的损失进行筛选，筛选出困难样本（即对分类和检测影响较大的样本），然后将筛选得到的这些样本应用在随机梯度下降中训练。在实际操作中是将原来的一个ROI Network扩充为两个ROI Network，这两个ROI Network共享参数。其中前面一个ROI Network只有前向操作，主要用于计算损失；后面一个ROI Network包括前向和后向操作，以hard example作为输入，计算损失并回传梯度。该算法在目标检测框架中被大量使用，如Fast RCNN．

* Focal Loss

  **易分样本（置信度高的样本）对模型的提升效果非常小，模型应该主要关注与那些难分样本**, 根据这个想法提出了Focal Loss, 思想就是把易分样本（高置信度样本）的损失分数降低，从而抑制易分样本的损失，可以看作一种软的重权重策略。Focal Loss 在标准交叉熵损失基础上修改得到的。这个损失函数可以通过减少易分类样本的权重，使得模型在训练时更专注于难分类的样本。

   

* GHM

### 目标检测中增强对小目标检测的方法

* 模型设计方面使用特征金字塔结构， 来增强网络对于多尺度尤其是小尺度特征的感知和处理能力； 尽可能地提升网络的感受野， 使得网络能够利用上下文信息来增强检测的效果； 同时减少网络总的下采样比例， 使最后用于检测的特征分辨率更高
* 在训练阶段， 可以提高小物体样本在总体样本中的比例； 也可以利用数据增强阶段， 将图像缩小易生成小物体样本
* 在测试阶段， 使用测试增强，使用更大的输入图像尺寸。

### 关于类别不平衡/长尾问题的处理方法

* 重采样
* 重加权

## 参考资料

* <https://zhuanlan.zhihu.com/p/60612064>
* <https://zhuanlan.zhihu.com/p/60698060>

