# 超参数调优(fine-tune hyper-parameter)

## 使用随机值

* try random values: don't use grids， 使用随机值，而不是使用网格
* coarse to fine, 粗糙到精细的搜索策略

## 使用合适的尺度

using an appropriate scale to pick hype-parameter

* linear scale: $l$, number layers of network
* log scale：$\alpha$, learning rate
* $\beta$, exponentially weight average: when $\beta$ is close to , the sensitivity of the results you get changes even with very small changes to $\beta$.

## 建议和技巧

tips and tricks for how to organize hyper-parameter search process

* re-test hyper-parameters occasionally
* babysitting one model, not a lot of computational resource(CPUs and GPUs)
* training many models in parallel

 