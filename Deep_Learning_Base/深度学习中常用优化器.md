# 深度学习中常用优化器

> 优化问题定义为寻找神经网络上的一组参数$\theta$, 能够显著地降低代价/损失函数$L(\theta)$, 该代价函数通常包括整个训练集上的性能评估和额外的正则化项。

​      深层神经网络的参数学习主要是通过梯度下降方法来寻找一组可以最小化结构风险的参数。

​	 为了方便下面进一步描述算法，下面定义几个常用符号(notation)

* $(x_k, y_k)$：表示训练集中第$k$个样本
* $\theta_t$： 在训练时间步$t$时的模型的参数值
* $L(\theta_t)$：在训练时间步$t$时的模型的损失函数值。
* $g(\theta_t)$：$g(\theta_t)=\nabla L(\theta_t)$, 在训练时间步$t$时的损失函数相对于参数的梯度值，用于更新$\theta_{t+1}$
* $\eta$：学习率/步长大小
* $m_{t+1}$：从第0时间步到第$t$时间步时的动量累计（一阶矩），用于计算$\theta_{t+1}$
* $s_{t+1}$：从第0时间步到第$t$时间步时的梯度历史平方和的累计（非中心二阶矩），用于计算$\theta_{t+1}$
* $\epsilon$：为了保持数值稳定性而设置的非常小的常数，一般取值$e^{-7}$ 到$e^{-10}$。  

## 梯度下降算法

根据训练过程中每一次迭代使用的数据量的大小， 可以分为批量梯度下降、随机梯度下降和小批量随机梯度下降等三种优化算法。根据不同的数据量和参数量，可以选择一种具体的实现形式。

#### 批量梯度下降(batch)

使用整个训练集的优化算法称为批量（batch）梯度或离线（off-line）算法。

#### 随机梯度下降(stochastic)

每次只使用单个样本的优化算法称为随机（stochastic）或在线（on-line）算法。

#### 小批量随机梯度下降(mini-batch  stochastic)

深度学习算法中一般介于上面两者之间，使用一个以上而又不是全部的训练样本，称为小批量（mini-batch）或小批量（mini-batch stochastic）算法。

* 更新公式,：

$$
\theta_t = \theta_{t-1} - \eta g_t \tag{1-1}
$$

* 特点

  SGD算法在每次迭代中，梯度下降根据自变量当前位置，沿着当前位置的梯度更新自变量。由于每次选取的样本数量比较少，损失会呈现震荡的方式下降。

## 梯度优化算法

动量算法通过梯度的[指数移动平均]( https://zhuanlan.zhihu.com/p/151786842)（累计了之前的梯度值）来代替每次的实际梯度， 缓解梯度震荡，从而加速学习。

#### SGDM(Momentum)

动量算法引入了动量$v$充当速度角色，它代表参数在参数空间移动的方向和速m率。速度被设为负梯度的指数移动平均。动量在物理上定义为质量乘以速度，这里我们假设是单位质量， 因此速度$m$也看作是粒子的动量。

**超参数$\gamma \in [0,1)$决定了之前梯度的贡献衰减得有多快​**， 当$\gamma=0$时，动量算法等价于小批量随机梯度下降。

* 更新公式
  $$
  \begin{align}
  m_t =\gamma& m_{t-1} - (1-\gamma)(\frac{\eta}{(1-\gamma)}) g_t = \gamma m_{t-1} - \eta g_t \tag{1-2} \\
  \theta_t =& \theta_{t-1} + m_t \tag{1-3} \\
  \end{align}
  $$

![image-20200719170205366](../graph/image-20200719170205366.png)

#### NAG(Nesterov Momentum Gradient)

Nesterov动量与标准动量得区别体现在梯度计算上。Nesterov 动量中，梯度计算在施加当前速度之后。

Nesterov 是 momentum算法得改进算法： Momentum保留了上一时刻的梯度 $g_{\theta_t}$，对其没有进行任何改变，NAG是在梯度更新时做一个矫正。

在动量法中，实际的参数更新方向$m_t$为上一步的参数更新方向$m_{t-1}$和当前梯度$g_t$的叠加。这样，$m_t$可以被拆分为两步进行，先根据$m_{t-1}$更新一次得到参数$\hat{\theta}$，再用$g_t$进行更新。  
$$
\begin{align}
\hat{\theta} =& \theta_{t-1} + \gamma m_{t-1}\tag{1-4} \\
\theta_{t} =& \hat{\theta} - \eta g_{t} \tag{1-5} \\
\end{align}
$$
**注意，这里$g_{t}$为梯度在时间步$t$的梯度，因此更合理的更新方向应该是$\hat{\theta}$上的梯度**。合并后的更新方向改变为 $\eta\nabla L(\theta_t + \gamma m_t)$。

* 更新公式
  $$
  \begin{align}
  m_t =& \gamma m_{t-1} - \eta\nabla L(\theta_{t-1} + \gamma m_{t-1}) = m_{t-1} - \eta g_{(\theta_{t-1} + \gamma m_{t-1})} \tag{1-4} \\
  \theta_t =& \theta_{t-1} + m_t \tag{1-5} \\
  \end{align}
  $$

![image-20200719165616043](../graph/image-20200719165616043.png)

* 物理意义

  Nesterov 动量算法的思想，可以理解为“超前部署”或“向前看一步”：

  第一步：在之前累计（accumulated）的梯度方向做一个大的跳跃（jump）。

  第二步： 然后再测量跳跃结束时的梯度，做一个修正。

  图中蓝色线为标准动量算法的更新方式：先计算当前时刻$t$梯度方向，然后再叠加时间步$t$累计的梯度方向，完成更新。

  图中棕色线、红色线、绿色线分别代表Nesterov 动量算法中的跳跃（jump）、梯度修正（correction）和梯度累加（accumulated gradient ）

##  自适应学习率优化算法

在梯度下降中，学习率$\eta$的取值非常关键，如果过大可能不会收敛，过小则收敛速度太慢。从经验上看，学习率在一开始要保持大些来保证收敛速度，在收敛到最优点附近时要小些以避免来回震荡。因此，比较简单直接的学习率调整可以通过学习率衰减（Learning Rate Decay）的方式来实现。  

常见得指数衰减做法有指数衰减，分段常数衰减等。除了固定衰减率的调整学习率方法外，还有些自适应地调整学习率的
方法，比如AdaGrad、 RMSprop、 AdaDelta等。  

#### AdaGrad

AdaGrad是借鉴L2正则化的思想，每次迭代时自适应地调整每个参数的学习率。

* 更新公式

AdaGrad算法会使用一个小批量随机梯度 $g_t$ 按元素平方的累加变量 $s_t$。在时间步0，AdaGrad将 $s_0$ 中每个元素初始化为0。在时间步 t ，首先将小批量随机梯度$s_t$按元素平方后累加到变量$s_t$
$$
s_t = s_{t-1} + g_t g_t \tag{2-1}
$$
接着，我们将目标函数自变量中每个元素的学习率通过按元素运算重新调整：
$$
\theta_{t} = \theta_{t-1} - \frac{\eta}{\sqrt{s_t+\epsilon}}g_t \tag{2-2}
$$

* 特点

  Adagrad算法的缺点是在经过一定次数的迭代依然没有找到最优点时，由于这时的学习率已经非常小，很难再继续找到最优点。  

### RMSProp

​	RMSProp[Hinton，2012]修改了AdaGrad 以在非凸设定下效果更好，改变梯度累计为指数加权的移动平均。AdaGrad 旨在应用于凸问题时快速收敛的问题。但是在训练非凸神经网络时， 学习轨迹可能穿过了很多不同的结构，最终达到一个局部为凸的区域而停止收敛。**这是由于AdaGrad 根据平方梯度的整个历史收缩学习率，可能使得学习率在达到这样的凸结构前就变得太小了，陷入局部最优。**

​	RMSProp使用指数（衰减）移动平均，相当于丢弃了较早时间步的历史，只使用距离当前时间步最近的固定大小的项进行学习率的更新，使其能够在找到局部凸结构后快速收敛。

* 更新公式

  给定超参数，权重因子$\gamma$
  $$
  s_t = \gamma s_{t-1} + (1-\gamma)g_t g_t \tag{2-3}
  $$
  接下来和AdaGrad算法一样，RMSProp算法将目标函数自变量中每个元素的学习率通过按元素运算重新调整，然后更新自变量
  $$
  \theta_{t} = \theta_{t-1} - \frac{\eta}{\sqrt{s_t+\epsilon}}g_t \tag{2-4}
  $$

* 特点

  RMSpop 的状态量$s_t$可以看作是最近$\frac{1}{1-\gamma}$个时间步的小批量随机梯度平方项的指数移动平均（关于移动平均可以参考[指数移动平均]([指数移动平均]( https://zhuanlan.zhihu.com/p/151786842))）， 这样参数参数的学习率在迭代过程中就不会一直降低，而是根据最近一段时间步的更新方向进行自适应地调整。

#### AdaDelta

AdaDelta算法[Zeiler, 2012]也是针对AdaGrad算法在迭代后期可能较难找到有用解的问题做了改进 。**AdaDelta算法没有学习率$\eta$​这一超参数。**

* 更新公式

  AdaDelta算法也像RMSProp算法一样，使用了小批量随机梯度$g_t$按元素平方的指数加权移动平均变量$s_t$，$s_t$的更新公式与$(2-3)$相同。
  $$
  s_t = \gamma s_{t-1} + (1-\gamma)g_t g_t \tag{2-5}
  $$
  AdaDelta算法引入了每次参数更新差$\Delta \theta$ 的平方的指数衰减权移动平均 $\Delta x$。  
  $$
  \Delta x_{t-1} = \gamma \Delta x_{t-2} + (1-\gamma) \Delta \theta \Delta \theta \tag{2-6}
  $$
  然后使用$\Delta x_{t-1}$来更新参数更新差$\Delta \theta_t$
  $$
  \Delta \theta_t = \sqrt{\frac{\Delta x_{t-1}+\epsilon}{s_t+\epsilon}}
  $$
  最后更新参数
  $$
  \theta_{t} = \theta_{t-1} - \Delta \theta_t
  $$

* 特点

  如不考虑ϵϵ的影响，AdaDelta算法与RMSProp算法的不同之处在于使用$\sqrt{\Delta x_{t-1}+\epsilon}$来替代学习率的超参数$\eta$。

  - 训练初中期，加速效果不错，很快。
  - 训练后期，反复在局部最小值附近抖动。

## 其他优化算法

#### Adam

自适应动量估计（Adaptive Moment Estimation， Adam）算法[Kingma and Ba，2014]是另一种自适应学习率算法，可以看作是动量法和 RMSprop 的结合，不但使用动量作为参数更新方向，而且可以自适应调整学习率。  **Adam = RMSProp  + Momentum**

* 更新公式

  给定移动平均衰减率超参数$\beta_1 \in(0, 1)$(算法建议为0.9)，，时间步t的动量变量$m_t$即小批量随机梯度$g_t$的指数加权移动平均(和动量法类似):
  $$
  m_{t} \leftarrow \beta_1 m_{t-1} + (1-\beta_1)g_t  \tag{3-1}
  $$
  给定移动平均衰减率超参数$\beta_2 \in(0, 1)$(算法建议为0.999)， 将小批量随机梯度按元素平方后的项$g_t g_t$做指数加权移动平均得到$s_t$(和RMSProp 算法类似):
  $$
  s_t \leftarrow \beta_2 s_{t-1} + (1-\beta_2)g_t g_t \tag{3-2}
  $$
  $m_t$以看作是梯度的均值（一阶矩）， $s_t$可以看作是梯度的未减去均值的方差（非中心二阶矩）  

  由于初始阶段$m_0$和$s_0$都被初始化为0，导致在迭代初期$m_t$和$s_t$ 的值会比真实的均值和方差要小。特别是当$\beta_1$和$\beta_2$ 都接近于1时，偏差会很大。因此，需要对偏差进行修正。(可以参考[指数移动平均]([指数移动平均]( https://zhuanlan.zhihu.com/p/151786842))中的证明过程)
  $$
  \begin{align}
  \hat{m_t} \leftarrow \frac{m_t}{1-\beta_1^t} \tag{3-3} \\
  \hat{s_t} \leftarrow \frac{s_t}{1-\beta_2^t} \tag{3-4}\\
  \end{align}
  $$
  接下来使用修正偏差后的$\hat{m_t}$和$\hat{s_t}$获取参数的更新差值
  $$
  \Delta \theta_t=  \frac{\eta}{\sqrt{\hat{s_t}}+\epsilon}\hat{m_t}
  $$
  最后更新参数
  $$
  \theta_t=\theta_{t-1} - \Delta \theta_t \tag{3-5}
  $$

#### Nadam

Adam算法是RMSProp与Momentum算法的结合，因此一种自然的Adam的改进方法是引入Nesterov加速梯度，称为Nadam算法 。 **Nadam = Adam  + Nesterov**

* 更新公式

  Nadam 与Adam 更新公式唯一的区别是一阶矩$\hat{m_t}$的修正公式， 其他公式都相同。
  $$
  \hat{m_t} = \frac{\beta_1}{1-\beta_1^{t+1}} + \frac{(1-\beta_1)}{1-\beta_1^{t}}g_t
  $$

## 可视化

![SGD optimization on loss surface contours](../graph/contours_evaluation_optimizers.gif)

![SGD optimization on saddle point](../graph/saddle_point_evaluation_optimizers.gif)
$$
\begin{align*}
\text{Vanilla SGD} \\
w_{t+1} &= w_t - \alpha \frac{\partial L}{\partial w_t} \\
\text{Momentum} \\
w_{t+1} &= w_t - \alpha m_t \\
m_t &= \beta m_{t-1} + (1 - \beta) \frac{\partial L}{\partial w_t} \\
\text{Adagrad} \\
w_{t+1} &= w_t - \frac{\alpha}{\sqrt{v_t + \epsilon}} \cdot \frac{\partial L}{\partial w_t} \\
v_t &= v_{t-1} + \bigg[\frac{\partial L}{\partial w_t}\bigg]^{2} \\
\text{RMSprop} \\
w_{t+1} &= w_t - \frac{\alpha}{\sqrt{v_t + \epsilon}} \cdot \frac{\partial L}{\partial w_t} \\
v_t &= \beta v_{t-1} + (1 - \beta) \bigg[\frac{\partial L}{\partial w_t}\bigg]^{2} \\
\text{Adadelta} \\
w_{t+1} &= w_t - \frac{\sqrt{D_{t-1} + \epsilon}}{\sqrt{v_t + \epsilon}} \cdot \frac{\partial L}{\partial w_t} \\
D_t &= \beta D_{t-1} + (1-\beta) [\Delta w_t]^2 \\
v_t &= \beta v_{t-1} + (1-\beta) \bigg[\frac{\partial L}{\partial w_t}\bigg]^{2} \\
\text{Nesterov} \\
w_{t+1} &= w_t - \alpha m_t \\
m_t &= \beta m_{t-1} + (1 - \beta) \frac{\partial L}{\partial w^*} \\
w^* &= w_t - \alpha m_{t-1} \\
\text{Adam} \\
w_{t+1} &= w_t - \frac{\alpha}{\sqrt{\hat{v_t}} + \epsilon} \cdot \hat{m_t} \\
\hat{m_t} &= \frac{m_t}{1-\beta_1^t} \\
\hat{v_t} &= \frac{v_t}{1-\beta_2^t} \\
m_t &= \beta_1 m_{t-1} + (1 - \beta_1) \frac{\partial L}{\partial w_t} \\
v_t &= \beta_2 v_{t-1} + (1 - \beta_2) \bigg[\frac{\partial L}{\partial w_t}\bigg]^{2} \\
\text{AdaMax} \\
w_{t+1} &= w_t - \frac{\alpha}{v_t} \cdot \hat{m_t} \\
\hat{m_t} &= \frac{m_t}{1-\beta_1^t} \\
m_t &= \beta_1 m_{t-1} + (1 - \beta_1) \frac{\partial L}{\partial w_t} \\
v_t &= \text{max}(\beta_2 v_{t-1}, \bigg|\frac{\partial L}{\partial w_t}\bigg|) \\
\text{Nadam} \\
w_{t+1} &= w_t - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \bigg(\beta_1 \hat{m}_{t} + \frac{1-\beta_1}{1-\beta_1^t} \cdot \frac{\partial L}{\partial w_t} \bigg) \\
\hat{m_t} &= \frac{m_t}{1-\beta_1^t} \\
\hat{v_t} &= \frac{v_t}{1-\beta_2^t} \\
m_t &= \beta_1 m_{t-1} + (1 - \beta_1) \frac{\partial L}{\partial w_t} \\
v_t &= \beta_2 v_{t-1} + (1 - \beta_2) \bigg[\frac{\partial L}{\partial w_t}\bigg]^{2} \\
\text{AMSGrad} \\
w_{t+1} &= w_t - \frac{\alpha}{\sqrt {\hat{v}_t} + \epsilon} \cdot m_t \\
m_t &= \beta_1 m_{t-1} + (1 - \beta_1) \frac{\partial L}{\partial w_t} \\
v_t &= \beta_2 v_{t-1} + (1 - \beta_2) \bigg[\frac{\partial L}{\partial w_t}\bigg]^{2} \\
\hat{v}_t &= \text{max}(\hat{v}_{t-1}, v_t)
%%\begin{tabular}{|l|c|c|c|}
%%\hline
%%\multicolumn{1}{|c|}{\textbf{Optimiser}} & \textbf{Year} & \textbf{Learning Rate} & \textbf{Gradient} \\ \hline
%%Nesterov                                 & 1983          &                        & \checkmark        \\ \hline
%%Momentum                                 & 1999          &                        & \checkmark        \\ \hline
%%AdaGrad                                  & 2011          & \checkmark             &                   \\ \hline
%%RMSprop                                  & 2012          & \checkmark             &                   \\ \hline
%%Adadelta                                 & 2012          & \checkmark             &                   \\ \hline
%%Adam                                     & 2014          & \checkmark             & \checkmark        \\ \hline
%%AdaMax                                   & 2015          & \checkmark             & \checkmark        \\ \hline
%%Nadam                                    & 2015          & \checkmark             & \checkmark        \\ \hline
%%AMSGrad                                  & 2018          & \checkmark             & \checkmark        \\ \hline
%\end{tabular}
\end{align*}
$$


SGDM

* large generalization gap
* stable
* better convergence(?)

Adam

* faster train,   

* large generalization gap
* unstable

## 参考资料

[An overview of gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/index.html)

[优化算法Optimizer比较和总结](https://zhuanlan.zhihu.com/p/55150256)

[一个框架看懂优化算法之异同 SGD/AdaGrad/Adam](https://zhuanlan.zhihu.com/p/32230623)

[动手学深度学习-优化算法](https://zh.gluon.ai/chapter_optimization/index.html)

<http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf>

[指数移动平均]([指数移动平均]( https://zhuanlan.zhihu.com/p/151786842))