# 模型量化调研

## 基本概念

量化的目的是将浮点型（float32）的数值从浮点型空间映射到整型（int8/int32）的空间。其中 `int8` 类型对应的取值范围为$(q_{min},q_{max})=(-2^{b-1},2^{b-1}-1)$，`uint8`类型对应的取值范围为$(q_{min},q_{max})=(0,2^{b}-1)$。

### 反量化

参考 [QAT](https://arxiv.org/pdf/1712.05877.pdf) 中的公式，首先引入**反量化（dequntization）**公式，可以表示为
$$
r = S(q-Z)
$$
其中 $r$ 表示FP32 的浮点数类型的真实值(**r**eal value)；$q$ 为INT8 的量化后的IN8/INT16 的量化值(**q**uantization value)；$S$表示缩放因子（**s**cale-factor）；$Z$ 表示零点(**z**ero-point)。

### 量化

对应的可以得出**量化（quantization）**公式如下
$$
q = \lfloor \frac{r}{S} + Z \rceil = round(\frac{r}{S} + Z)
$$
参数含义，其中$\lfloor \cdot \rceil$表示将浮点数**舍入(round)**到最接近的整数。**量化中的误差主要是由于舍入（round）操作引起的**

容易看出，**量化的目的本质是确定$S$ 和 $Z$ 这两个参数，这两个参数直接决定了量化的精度**。当确定了$S$ 和$Z$我们就可以执行最后的量化操作，完成量化。

假设浮点类型参数的取值空间为$[r_{min}, r_{max}]$，量化后参数的取值空间为$[q_{min}, q_{max}]$，缩放因子$S$ 可以表示为
$$
S = \frac{r_{max} - r_{min}}{q_{max} - q_{min}}
$$
进一步容易得到，零点的公式
$$
Z = -\frac{r_{min}}{S} + q_{min}
$$

在实际操作过程中，浮点数的值$r$ 可能会超过浮点数的取值范围$[r_{min}, r_{max}]$, 对应的量化后数值也可能会超过的取值范围$[q_{min}, q_{max}]$，**具有固定类型精度的编程语言会裁剪（clip/clamp）超出范围的值**。具体来说，量化过程中会增加额外的裁剪步骤。引入裁剪操作的量化公式如下
$$
q = clip(round(\frac{r}{S}+Z); q_{min}, q_{max})
$$
其中裁剪公式
$$
clip(x;l,u)=\begin{cases}
l \quad if\ x \leq l \\
x \quad if\ l< x < u \\
u \quad if\ x \geq u \\
\end{cases}
$$

### 对称量化和非对称量化

* 非对称量化

  非对称量化(Asymmetric quantization) 通过三个参数定义，缩放因子s，零点z和位宽b 确定，即
  $$
  q = clamp(\lfloor\frac{r}{s}\rceil + z; q_{min},q_{max})
  $$

  $$
  r = s(q-z)
  $$

  非对称量化中缩放因子计算公式如公式$(3)$，零点计算公式如公式$(4)$（实际应用中可能有差异）

* 对称量化

  对称量化(symmetric quantization) 可以看作是普通非对称量化的简化版本，**对称量化限制零点 $z$ 为 0**。对称量化减少了处理零点偏移的计算损耗。但是缺乏偏移因子，这也限制了整数域和浮点数域之间的映射。

  对称量化的公式表示如下
  $$
  q = clamp(\lfloor\frac{r}{s}\rceil; q_{min},q_{max})
  $$

  $$
  r = sq
  $$

  特别地，对于有符号(sign)量化和无符号(unsign)量化可以分别表示为
  $$
  q = clamp(\lfloor\frac{r}{s}\rceil; 0,2^b-1)
  $$

  $$
  q = clamp(\lfloor\frac{r}{s}\rceil; -2^{b-1},2^{b-1}-1)
  $$

  对称量化中缩放因子的计算公式可以表示如下（实际应用中可能有差异）
  $$
  s = \frac{max(abs(r_{min}),abs(r_{max})) \times2}{q_{max}-q_{min}}
  $$

* 二次幂量化

  二次幂(power-of-two)量化是对成量化的一种特殊情形，它的尺度因子$s$被限制为二次幂$2^k$。二次幂量化使用的缩放因子$s$对应简单的移位操作，可以提升硬件的效率。但是受限于缩放因子$s$的表现能力，使得舍入和裁剪误差之间的权衡变得复杂。

![quantization-symmetric-asynmmetric](../graph/image-20230320201147317.png)

### 小结

量化(quantization)公式可以表示为
$$
f_q(r, S, Z) = clip(round(\frac{r}{S}+Z), q_{min}, q_{max})
$$
反量化(de-quantization)公式可以表示为
$$
f_{d}(q,S,Z) =S(q-Z)
$$

## 量化方法

### PTQ量化

* 量化流程

  ```mermaid
  graph TD
  
  A(预训练) -->B(校准)
  B --> |执行量化| C(模型量化)
  C --> D(模型导出)
  ```

  

  1. 预训练：预训练首先以FP32 精度训练模型，得到预训练模型。

  2. 校准(calibration)

     校准具体可以分为两步

     1. 使用小部分数据对FP32模型进行校准，统计网络各层的权重和激活的数据分布（最大最小值）
     2. 使用数据分布特性，计算各层的缩放因子$S$和零点参数$Z$。

  3. 模型量化：使用校准得到的量化参数对FP32模型的各层进行量化。

  4. 模型导出：将模型导出为 onnx 

PQT 量化对于达模型来说很有效，但是对于小模型会导致准确度的显著下降。

### QAT 量化

神经网络中使用量化会引入信息损失，因此量化后的整数模型的精度一般会低于浮点数模型。**这种信息丢失是由于浮点数经过量化和反量化是不能够完全恢复的导致的**。这可以公式化为
$$
x = f_d(f_q(x, s_x, z_x), s_x, z_x) + \Delta x
$$
其中$\Delta x$ 是一个未知的很小的值，表示偏差。如果$\Delta x=0$表示量化后的整数模型的精度会与浮点数模型的精确度完全相同。

[QAT](https://arxiv.org/pdf/1712.05877.pdf)  量化算法由Google 提出，QAT 的思想是要求神经网络在训练期间考虑这种信息丢失造成的影响。通过在训练期间**模拟前向过程中量化的效果**，模型在训练过程中逐渐适应这种量化损失，使得模型在推理过程中损失最小。

具体来说，QAT 是在模型训练时为每一个变量（权重、激活等）加入量化（quantization）和反量化层（de-quantization），称为**伪量化（fake quantization）**节点，模拟量化引起的误差。可以公式化为
$$
\begin{align*}
\hat{x} &= f_d(f_q(x, s_x, z_x), s_x, z_x) \\
&=s_x(clamp(round(\frac{x}{s_x} + z_x);x_{min}, x_{max})-z_x)
\end{align*}
$$

QAT 训练过程中尺度因子$S$ 和零点$Z$ 会被收集，用于最后的量化操作。 


* 伪量化节点

  假设浮点型参数的取值范范围为$[a, b]$，通过量化级别的数量和范围截断，点级量化过程可以参数化为如下公式：

  首先对真实的浮点数值进行裁剪（clamb/clip）操作
  $$
  clamb(r;a,b) := min(max(r,a),b)
  $$
  结合公式$(3)$ 可以得到缩放因子
  $$
  s(a,b,n):= \frac{b-a}{n-1}
  $$


  式中 $n$ 表示量化级别的数量，如8比特量化时$n=2^8=256$。

  结合公式$(2)$ 和 $(1)$ 分别执行量化（quantization) 和反量化（quantization）操作得到**引入模拟量化操作后的浮点数值**。


$$
q(r:a,b,n) = \lfloor \frac{clamb(r;a,b)-a}{s(a,b,n)} \rceil s(a,b,n) + a
$$
容易得出，**每一个伪量化节的实质是量化-伪量化操作组合，其中$\lfloor \frac{clamb(r;a,b)-a}{s(a,b,n)} \rceil$ 模拟量化过程，$(\cdot)s(a,b,n)+a$ 表示反量化过程，从而实现在模型中模拟量化操作**

**伪量化节点的**会收集缩放因子$S$和零点$Z$的值。**在微调过程中，各层伪量化节点中的缩放因子$S=s(a,b,n)$ 和零点 $Z=z(a,b,n)$会得到估计和更新**

* QAT量化过程

  ![qt-fake-qt](../graph/image-20230302162119468.png)

  上图分别表示推理和训练时的计算图。

  在训练/微调过程中，所有的变量和节点都使用32位浮点进行运算。在需要量化的参数后面插入**伪量化节点**（上图中分别对应 wt quant 和 act quant），来模型量化的效果。同时使用常规的优化算法进行训练。

  在推理过程中，推理框架根据不同类型计算类型可能会对应不同的计算精度。其中卷积和激活算子采用INT8精度；偏置加法只涉及INT32精度。

  **伪量化节点可以看作是通过模型模型量化过程中的舍入取整操作引起的误差，并将这种误差看作一种训练噪声。通过fine-tune过程，让模型去适应这种噪声。从而在模型量化位IN8时，减少由于量化操作造成的精度损失**

* 量化流程

  ```mermaid
  graph TD
  
  A(预训练) -->B(插入伪量化节点)
  B -->  C[微调 QAT 模型]
  C --> |存储量化参数|D(模型量化)
  D --> E(模型导出)
  ```

  1. 预训练：预训练首先以FP32 精度训练模型，得到预训练模型。
  2. 插入伪量化节点：在预训练模型中插入伪量化节点，得到QAT 模型
  3. 微调 QAT 模型：估计和更新各层伪量化节点中的缩放因子$S$ 和零点$Z$等量化参数
  4. 模型量化：使用得到的量化参数，对FP32模型执行量化操作得到量化模型
  5. 模型导出：将模型导出为 onnx 

## 参考资料

* <https://leimao.github.io/article/Neural-Networks-Quantization/#Quantization>
* <https://zhuanlan.zhihu.com/p/548174416>
* <https://pytorch.org/blog/quantization-in-practice/>
* <https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html>
* [Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inferencen](https://arxiv.org/abs/1712.05877)
* [A White Paper on Neural Network Quantizatio](https://arxiv.org/abs/2106.08295)